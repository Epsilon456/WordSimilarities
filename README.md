# WordSimilarities
This project demonstrates how to use various metrics of text similarity to analyze course descriptions on USC's course catalog 
found here: (https://classes.usc.edu/term-20193).  A scond script compares a given USC course to the most similar UCLA course (from the UCLA course catalog: https://www.registrar.ucla.edu/Academics/Course-Descriptions)

# Installing
This project relies upon the scikit-learn and gensim libraries.  All requirements can be found in the "requirements.txt" file. 
A python environment with all necessary libraries can be created using the following conda commands:

```
conda create -y --name WordSimilarities python==3.6
conda activate WordSimilarities 
conda install --file "path\to\requirements.txt"
```
# Running the code
The method comparison code can be run by calling Main.py.  When run, the code will automatically determine if a checkpoint files generated by a previous
run of the scraper and cleaner.  If these files are not found, then it will automatically scrape and clean.  If these files are found, 
the console will prompt the user to either re-scrape or re-clean and overwrite these files.  The user is prompted to respond by entring 
"1" into the console for "yes" and "0" for "no".  (Press 'enter' after entering a response).

The "USCtoUCLA.py" code runs a separate set of functions that will return a UCLA course closest to a provided USC course. When this code is run, it will automatically determine if the UCLA site needs to be scraped or if it can revert to a previously saved checkpoint (json) file. After this, it will initialize the wordSim method (as the results show that it was the most accurate of the 5.)  The console will then prompt the user to press any key to display the next course.  Each time the user presses a key, the code will randomly select a USC course and then determine the most similar UCLA course.  The resulting course names and descriptions will be printed to the console.  Afterwards, the prompt will repeat 10 times.  (The user may press 'q' to exit from this loop.)

# Methods
## There are 5 methods used to determine how similar two courses are.

### Jacard Similarity
This method will calculate the Jacard similarity between two course descriptions.  This is done by creating a set of words for each 
description.  The similarity score will be the number of words common to both sets divided by the number of unique words contained in 
the union of both sets.

### Leveshtein Distance Similarity
This method calculates the Levenshtein distance between two course names.  The Leveshtein Distance can be thought of the number of 
insertions, deletions, or replacements required to convert one string into another string.  Intuitively, this is an integer used to compare
two strings where the higher the value, the more different the strings are. A Similarity score can be obtained from this by taking the 
compliment of the normalized Levenshtein Distance.             

First, the Levenshtein Distance is normalized by dividing the distance by the length of the longer of the two strings. Next, the 
compliment can be found by subtracting this value from 1.0.  This creates a similarity score in between 0 and 1 where a value of 1 
represents two identical strings.

### WordSim
The "WordSim" method uses a Word2Vec embedding of each description.  This embedding takes the form of a list of 
high dimensional vectors where each vector represents a word in the description.  Each description, therefore, can be thought of as a set
of vectors.  Taking the average vector of each set can represent each description text as a single vector. As a result, two course
descriptions can be compared by taking the cosine similarity between these two vectors.  The similarity will be between 0 and 1 where
a value of 1 represents vectors which are more similar.

### DocSim
This method is similar to the "WordSim" method, however, it uses a Doc2Vec embedding which represents the entire description as a single 
vector. The similarity of two course description texts is then found by the cosine similarity between these two vectors.

### GloveSim
The GloveSim method uses pretrained "GloVe" embeddings which are taken from gensim. These embeddings represent each word of a description
as a unique vector in much the same way that "WordSim" does. However, unlike "WordSim" instead of taking an average, a special mthod is used
to compare descriptions.

This method envolves concatenating four vectors to create a matrix.  
1) An average of all word vectors in the description(exactly like "WordSim").
2) The standard deviation of all word vectors added to the average vector.
3) The maximum value for each dimension across all word vectors.
4) The minimum value for each dimension across all word vectors.

With each description now represented as a matrix, the cosine similarity can be each individual vector from each description. This gives
4 different similarity scores which can then be averaged to create one, single score.

# Metrics
In order to determine how well each method is performing at matching classes, two metrics are used.
First, a course is fed into a scoring method which will then assign a similarity score to each course in a dataframe. These methods will 
generate a dataframe containing course names as well as a similarity score (similarity relative to the course that was fed into the method).
## School Name metric
A good assumption is that two courses which are similar will be from the same school.  For example, "Intro to Mechanical Engineering" will
be more similar to"Intro to Mechanical Design" than "Intro to Academic Medicine".  To create a baseline metric, the school that the input
course is part of will be compared to the school of each of the top 10 most similar (as determined by the method) courses.  Taking
the percentage of the top 10 courses that are part of the same school as the input course gives a good baseline metric for how well the method
is performing.

## Prerequisite scoring
Another way to score each method is to assume that similar courses will have the same prerequisites. Taking the top 3 most similar
courses, this method counts how many prerequisite courses these top 3 have in common with the input course. If the input course has no 
prerequisites, then the comparison will be given a score of 1 for each course in the top three that also has no prerequisite or a score of 
0 for each course in the top 3 that contains at least one prerequisite.  The nuber of correct prerequisites is then averaged to produce a
number between 0 and 1.

# File explainations
The file method comparison program can be run by calling "Main.py" which will, in turn, call the supporting files.  The main flow of the 
code is as follows:  
1) It will scrape USC's course catalog and save the relevant data includign course numbers and course descriptions.
2) The raw data will be cleaned and standardized
3) The data will be broken into a train and test dataset.
4) The training data will be used to train the word2vec and doc2vec embeddings.  These will also act as a reference list to which the 	    courses from the test data set will be compared.
5) Since each course in the reference list will be compared to each test course, a dictionary is built up which saves the course 	    embeddings for each method for each course.
6) A list of courses from the test dataset will be fed into 5 different scoring functions which will generate a scored list of courses
  from the test data set which are closest to the initial dataset.
7) Each scoring function will be be given an accuracy score using two metrics.
8) A cross validation loop will run and produce a dataframe which compares the accuracy of each of the 5 methods using each of the two metrics.

The UCLA comparision program can be called by simply running "USCtoUCLA.py".  This will do the following:
1) Scrape UCLA's site if a checkpoint file is not found
2) Clean the raw data (only if a checkpoint file was not found in step 1)
3) Load the UCLA data as a training set to the WordSim method
4) Randomly select 10 USC courses
5) Prompt the user to pass a random USC course to a function which will return the closest UCLA course.
6) Display course names and descriptions for both courses.

## Setup
This file constructs the file pathways used throughout the code.  There are three particular steps which take a long time and only 
need to be run once.  The code will automatically save the results of these three steps so that they will not need to be rerun the next
time the code is executed.  The setup file will automatically place these files in the folder that the script is being run from, however
the user may change this by adjusting this file.

The three intermediate steps which are checkpointed are as follows:
1) After the website is scraped, the raw data is saved as a list of dictionaries in a pickle format. 
2) After cleaning the data, it is saved in a json format.
3) One of the 5 methods uses GloVe embeddings which are downloaded using gensim's downloader api. Since this download is relatively large,
  this step is pickled.
  
Each step that uses checkpointing will check to determine if the checkpoint file exists. If so,
then it will load the checkpoint file and skip the steps used to create it.

Note: Python's pickle format is a simple way of storing multiple data types and has a quick read and write time.
Loading pickle files, is generally seen as a security risk if loaded from an unknonwn source. In this project, however,
all pickle files are generated by the code the first time it is run, thus making their source known.

### Main
This file serves to tie all supporting scripts together. The main tasks handled by this script invovle 
splitting the dataset into training and testing datasets and performing the cross validation.

### USCCrawler2
This script will scrape USC's site and save data from the relevant courses. The scraper is set to 
look at every course offered by the Engineering and Medicine schools. The following data will be saved
from for each course:

Course Number
Course Name
Description
Prerequisites
Prerequisite links
School (either "Engineering" or "Medicine")

### UCLAScraper
This script will scrape UCLA's site and save the results as a json file.

### Cleaner
This script cleans the raw data to make comparisions standardized.  The script set all text to lowercase
and then remove stop values.  Stop values include punctuation, common words (such as "a","to", "of",etc.),
and certain regular expressions.

Cleaned data will then be saved in a json format.

### UCLACleaner

This script will clean the raw UCLA data in a similar was as "Cleaner."  It will also save a copy of the cleaned data to a json file.


### Similarities
The most important functions in this code are found in this script.  All 5 methods used for determining
similarity exist inside the "Similarities" class.  When instantiated with a training dataframe, the "__init__()"
function will call methods used to train the word2vec and doc2vec models to create word embeddings.
Afterward, the similarity methods can be called by passing a test dataframe as well as two course numbers.

Several helper methods are used within this class (both for initialization and for repedative calculations).
All helper methods are preceeded with an underscore.
### Score
The scoring script contains functions which call the methods outlined in the "Similarities" class.
These functions are used to apply the similarity methods across a dataset and to score the methods
with two accuracy score functions.
# Results
The table below shows the results from an example output.  THe WordSim model outperforms all other methods by both metrics.  It is understandable that the WordSim model outperforms the Jacard and Lev models, since wordsim uses a deeper understanding of the course descriptions.  (The Lev model for example, only looks at course names and does not awknowledge descriptions at all).  Interestingly, however, the other two embedding models (DocSim and GloveSim) underperform all others.  The low performance of the GloveSim model could be explained by the fact that it was not trained with domain specific data as the WordSim model was.  The low performance of the DocSim model, however, does not have an obvious explaination.

In general, the prerequisite scores for each method are nearly the same (with the exception of docsim performing poorly).  The prerequisites may not be the best metric since not all courses have prerequisites.  Courses that do have prerequisites may not have a similar course that has the exact same prerequisits.  As a result, most of the time, the prerequsites for a given course and the most similar course will not match (if they have prerequisits to begin with).  
            School      Preq
Jacard    0.762222  0.497531
Lev       0.730000  0.475926
WordSim   0.820000  0.517284
DocSim    0.592222  0.444444
GloveSim  0.598889  0.503704

# Author
Arren Bustamante
